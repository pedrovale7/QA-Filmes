import os
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from llama_index.llms.groq import Groq

from vetorizacao import vetorizar_texto, busca_vetorial

load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

app = FastAPI(title="API Q&A Filmes")
# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vite default ports
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
buscador,llm = None, None

class ChatRequest(BaseModel):
    pergunta: str

@app.on_event("startup")
def iniciar_sist():
    global llm
    print("--- üöÄ TENTANDO INICIAR O SISTEMA ---")
    
    # 1. Verifica se a chave foi lida do arquivo .env
    if not GROQ_API_KEY:
        print("‚ùå ERRO CR√çTICO: A vari√°vel GROQ_API_KEY est√° vazia ou None!")
        print("   -> Verifique se o arquivo .env existe na mesma pasta.")
        print("   -> Verifique se tem algo escrito dentro dele.")
        return

    print(f"üîë Chave encontrada: {GROQ_API_KEY[:5]}... (oculto)")

    try:
        # 2. Tenta conectar na Groq
        print("üîå Conectando aos servidores da Groq...")
        llm = Groq(model='llama-3.1-8b-instant', api_key=GROQ_API_KEY)
        
        # 3. Teste r√°pido para ver se a chave funciona de verdade
        # Fazemos uma pergunta boba s√≥ pra testar a conex√£o
        teste = llm.complete("Diga oi")
        print(f"‚úÖ CONEX√ÉO BEM SUCEDIDA! O Groq respondeu: '{teste.text.strip()}'")
        
    except Exception as e:
        print(f"\n‚ùå ERRO FATAL AO CONECTAR NO GROQ:")
        print(f"   -> {str(e)}")
        print("   -> Verifique sua internet ou se a chave API √© v√°lida.")
        llm = None

@app.post("/chat")
def chat_endpoint(request: ChatRequest):

    if llm is None:
        raise HTTPException(status_code=500, detail="O modelo LLM n√£o foi carregado.")
    
    pergunta_usuario = request.pergunta.lower().strip()
    
    try:
        respostas = busca_vetorial(pergunta_usuario, top_k=5)
        contexto = "" 

        for filme in respostas:

            titulo = filme.get('title') or filme.get('Nome_Filme') or "N/A"
            genero = filme.get('genres') or filme.get('Generos') or "N/A"
            nota = filme.get('media_nota') or filme.get('Nota') or "N/A"
            detalhes = filme.get('contexto_completo') or filme.get('Detalhes') or ""

            trecho = f"- Filme: {titulo} | G√™neros: {genero} | Nota: {nota}\n"
            trecho += f"  Detalhes: {detalhes}\n\n"
            

            contexto += trecho
        
        prompt = f"""
        Voc√™ √© um especialista em filmes. Use apenas os dados abaixo para responder perguntas do usu√°rio sobre sugest√µes de filmes
        Tenha uma comunica√ß√£o leve e amig√°vel e recomende os filmes listados explicando o porque da sua desic√£o.

        REGRAS DE RESPOSTA:
        1. N√ÉO fa√ßa introdu√ß√µes longas (como "Ol√°", "√ìtima escolha").
        2. Limite a resposta a no m√°ximo 2 ou 3 frases por filme.
        3. Diga apenas: Nome do Filme + Nota + Motivo breve da recomenda√ß√£o.
        4. Se houver mais de um filme, separe claramente.

        SISTEMA: {contexto}
        PERGUNTA DO USU√ÅRIO: {pergunta_usuario}"""
        
        resposta = llm.complete(prompt)
        
        return {"resposta": resposta.text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))